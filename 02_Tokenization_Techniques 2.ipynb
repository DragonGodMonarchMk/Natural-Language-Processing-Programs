{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cf738a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdcad7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize text with specified stopwords as delimiters\n",
    "def tokenize_with_stopwords(text, stopwords):\n",
    "    # Create a regular expression pattern\n",
    "    pattern = re.compile(r'\\b(?:' + '|'.join(stopwords) + r')\\b|\\b\\w+\\b')\n",
    "    \n",
    "    # Tokenize the text using the pattern\n",
    "    tokens = pattern.findall(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad1bf73",
   "metadata": {},
   "source": [
    "##### This line constructs a regular expression pattern. Let's break it down:\n",
    "    ### \\b: This denotes a word boundary.\n",
    "    ### (?: ... ): This is a non-capturing group that allows grouping of the specified stopwords.\n",
    "    ### |: This is the alternation operator, which acts like a logical OR.\n",
    "    ###' '.join(stopwords): This joins the stopwords with the alternation operator, creating a pattern like word1|word2|word3.\n",
    "    ###\\b: Another word boundary.\n",
    "    ###|\\b\\w+\\b: This part of the pattern matches any word (sequence of word characters) that is not in the stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea670098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for demonstration\n",
    "sample_text = \"This is a sample text, and it was written to demonstrate tokenization with stopwords.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "664271b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords\n",
    "stopwords = [\"is\", \"the\", \"was\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "881f0b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sample text with stopwords as delimiters\n",
    "tokens = tokenize_with_stopwords(sample_text, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11dd5a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['This', 'is', 'a', 'sample', 'text', 'and', 'it', 'was', 'written', 'to', 'demonstrate', 'tokenization', 'with', 'stopwords']\n"
     ]
    }
   ],
   "source": [
    "# Print the tokens\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb06cb5",
   "metadata": {},
   "source": [
    "## Using a Text File as Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60cdfc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8470e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for tokenization\n",
    "def tokenize_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file: #uni-code transformation format\n",
    "        text = file.read()\n",
    "        tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76cc98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your_file.txt' with the path to your text file\n",
    "file_path = 'sample_text.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "229f171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the content of the file\n",
    "tokens = tokenize_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a27f46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['The', 'Taj', 'Mahal', 'complex', 'is', 'believed', 'to', 'have', 'been', 'completed', 'in', 'its', 'entirety', 'in', '1653', 'at', 'a', 'cost', 'estimated', 'at', 'the', 'time', 'to', 'be', 'around', '₹32', 'million', ',', 'which', 'in', '2023', 'would', 'be', 'approximately', '₹35', 'billion', '(', 'US', '$', '498', 'million', ')', '.', '[', '8', ']', 'The', 'construction', 'project', 'employed', 'some', '20,000', 'artisans', 'under', 'the', 'guidance', 'of', 'a', 'board', 'of', 'architects', 'led', 'by', 'Ustad', 'Ahmad', 'Lahori', ',', 'the', 'emperor', \"'s\", 'court', 'architect', '.', 'Various', 'types', 'of', 'symbolism', 'have', 'been', 'employed', 'in', 'the', 'Taj', 'to', 'reflect', 'natural', 'beauty', 'and', 'divinity', '.', 'The', 'Taj', 'Mahal', 'was', 'designated', 'as', 'a', 'UNESCO', 'World', 'Heritage', 'Site', 'in', '1983', 'for', 'being', '``', 'the', 'jewel', 'of', 'Islamic', 'art', 'in', 'India', 'and', 'one', 'of', 'the', 'universally', 'admired', 'masterpieces', 'of', 'the', 'world', \"'s\", 'heritage', \"''\", '.', 'It', 'is', 'regarded', 'by', 'many', 'as', 'the', 'best', 'example', 'of', 'Mughal', 'architecture', 'and', 'a', 'symbol', 'of', 'India', \"'s\", 'rich', 'history', '.', 'The', 'Taj', 'Mahal', 'attracts', 'around', '5', 'million', 'visitors', 'a', 'year', ',', '[', '3', ']', 'and', 'in', '2007', 'it', 'was', 'declared', 'a', 'winner', 'of', 'the', 'New', '7', 'Wonders', 'of', 'the', 'World', '(', '2000–2007', ')', 'initiative', '.']\n"
     ]
    }
   ],
   "source": [
    "# Print the tokens\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45697c3",
   "metadata": {},
   "source": [
    "## Various Tokenization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b77ab28",
   "metadata": {},
   "source": [
    "### 1. Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7a4abef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['Word', 'tokenization', 'is', 'important', 'for', 'natural', 'language', 'processing', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Word tokenization is important for natural language processing.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Word Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8eb828",
   "metadata": {},
   "source": [
    "### 2. Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "071f6340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['Sentence tokenization breaks text into sentences.', 'This is an example.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Sentence tokenization breaks text into sentences. This is an example.\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentences:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097fdcc5",
   "metadata": {},
   "source": [
    "### 3. Whitespace Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "218c0e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokens: ['Whitespace', 'tokenization', 'splits', 'text', 'based', 'on', 'spaces.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Whitespace tokenization splits text based on spaces.\"\n",
    "tokens = text.split()\n",
    "print(\"Whitespace Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc4256a",
   "metadata": {},
   "source": [
    "### 4. Custom Delimiter Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4091854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Delimiter Tokens: ['Custom', 'delimiter', 'tokenization', '', 'separating', 'items:', 'apple', '', 'orange', '', 'banana.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Custom delimiter tokenization, separating items: apple, orange, banana.\"\n",
    "tokens = re.split(r'[,\\s]', text)\n",
    "print(\"Custom Delimiter Tokens:\", tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
