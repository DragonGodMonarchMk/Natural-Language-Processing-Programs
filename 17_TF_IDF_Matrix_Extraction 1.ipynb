{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9MJ5xxlaH-nt",
    "outputId": "c3128661-8e79-49ba-f7bc-87129bc52f3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix (Using NLTK and Python):\n",
      " [[ 0.          0.          0.06757752  0.06757752 -0.09589402  0.06757752\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.         -0.09589402  0.\n",
      "   0.06757752  0.          0.          0.          0.06757752  0.06757752]\n",
      " [ 0.          0.05792359  0.          0.         -0.08219488  0.\n",
      "   0.          0.05792359  0.05792359  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\"The cat sat on the mat\", \"The dog jumped over the fence\", \"The cat and the dog are friends\"]\n",
    "\n",
    "# Step 1: Tokenize the documents\n",
    "tokenized_docs = [nltk.word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "# Step 2: Compute TF (Term Frequency) - no. of times a word occurs / total number of words\n",
    "def compute_tf(tokens):\n",
    "    tf = {} # word: term frequency\n",
    "    total_words = len(tokens)\n",
    "    for word in tokens:\n",
    "        tf[word] = tf.get(word, 0) + 1 / total_words\n",
    "    return tf\n",
    "\n",
    "tf_matrices = [compute_tf(doc) for doc in tokenized_docs]\n",
    "\n",
    "# Step 3: Compute IDF (Inverse Document Frequency) - log (total no. of documents / no. of docs in which term appears)\n",
    "def compute_idf(docs):\n",
    "    idf = {} # word: IDF Scores\n",
    "    total_docs = len(docs)\n",
    "    all_words = set(word for doc in docs for word in doc)\n",
    "    for word in all_words:\n",
    "        doc_count = sum(1 for doc in docs if word in doc)\n",
    "        idf[word] = np.log(total_docs / (1 + doc_count))\n",
    "    return idf\n",
    "\n",
    "idf = compute_idf(tokenized_docs)\n",
    "\n",
    "# Step 4: Compute TF-IDF\n",
    "def compute_tfidf(tf_matrix, idf):\n",
    "    tfidf = {} # word: TF-IDF Scores\n",
    "    for word, tf in tf_matrix.items():\n",
    "        tfidf[word] = tf*idf[word]\n",
    "    return tfidf\n",
    "\n",
    "tfidf_matrices = [compute_tfidf(tf, idf) for tf in tf_matrices]\n",
    "\n",
    "# Step 5: Convert TF-IDF into matrix format (row represent documents and columns represents words)\n",
    "def vectorize(documents, idf):\n",
    "    vocab = list(idf.keys()) # no. of unique words\n",
    "    matrix = np.zeros((len(documents), len(vocab)))\n",
    "    for i, doc in enumerate(documents):\n",
    "        for j, word in enumerate(vocab):\n",
    "            matrix[i, j] = tfidf_matrices[i].get(word, 0)\n",
    "    return matrix\n",
    "\n",
    "tfidf_matrix = vectorize(documents, idf)\n",
    "\n",
    "print(\"TF-IDF Matrix (Using NLTK and Python):\\n\", tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AP3TuUx5ITtC",
    "outputId": "4659fde5-0740-4feb-8ae9-be099aa613ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix (Using TfidfVectorizer):\n",
      " [[0.         0.         0.34101521 0.         0.         0.\n",
      "  0.         0.44839402 0.44839402 0.         0.44839402 0.52965746]\n",
      " [0.         0.         0.         0.34101521 0.44839402 0.\n",
      "  0.44839402 0.         0.         0.44839402 0.         0.52965746]\n",
      " [0.42439575 0.42439575 0.32276391 0.32276391 0.         0.42439575\n",
      "  0.         0.         0.         0.         0.         0.50130994]] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dell\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog jumped over the fence\",\n",
    "    \"The cat and the dog are friends\"\n",
    "]\n",
    "\n",
    "# Step 1: Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer = nltk.word_tokenize)\n",
    "\n",
    "# Step 2: Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the sparse matrix to dense array for better visibility\n",
    "dense_tfidf_matrix = tfidf_matrix.toarray()\n",
    "\n",
    "print(\"TF-IDF Matrix (Using TfidfVectorizer):\\n\", dense_tfidf_matrix, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoDXxfhSIkpI",
    "outputId": "a0d4e47e-dac9-4a67-db26-3548313d5490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix (Using spaCy and scikit-learn):\n",
      " [[0.         0.         0.34101521 0.         0.         0.\n",
      "  0.         0.44839402 0.44839402 0.         0.44839402 0.52965746]\n",
      " [0.         0.         0.         0.34101521 0.44839402 0.\n",
      "  0.44839402 0.         0.         0.44839402 0.         0.52965746]\n",
      " [0.42439575 0.42439575 0.32276391 0.32276391 0.         0.42439575\n",
      "  0.         0.         0.         0.         0.         0.50130994]]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog jumped over the fence\",\n",
    "    \"The cat and the dog are friends\"\n",
    "]\n",
    "\n",
    "# Initialize spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Tokenize and lemmatize the documents using spaCy\n",
    "tokenized_docs = []\n",
    "for doc in documents:\n",
    "    tokens = [token.lemma_ for token in nlp(doc)]\n",
    "    tokenized_docs.append(\" \".join(tokens))\n",
    "\n",
    "# Use TfidfVectorizer for TF-IDF matrix extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents to compute the TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(tokenized_docs)\n",
    "\n",
    "# Convert the sparse matrix to dense array for better visibility\n",
    "dense_tfidf_matrix = tfidf_matrix.toarray()\n",
    "\n",
    "print(\"TF-IDF Matrix (Using spaCy and scikit-learn):\\n\", dense_tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5le-8eXYJOR3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
